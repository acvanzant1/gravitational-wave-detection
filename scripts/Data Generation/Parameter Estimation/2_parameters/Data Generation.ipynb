{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "########### IMPORTS ############\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamvanzant/.pyenv/versions/gw_env_39/lib/python3.9/site-packages/pycbc/types/array.py:36: UserWarning: Wswiglal-redir-stdio:\n",
      "\n",
      "SWIGLAL standard output/error redirection is enabled in IPython.\n",
      "This may lead to performance penalties. To disable locally, use:\n",
      "\n",
      "with lal.no_swig_redirect_standard_output_error():\n",
      "    ...\n",
      "\n",
      "To disable globally, use:\n",
      "\n",
      "lal.swig_redirect_standard_output_error(False)\n",
      "\n",
      "Note however that this will likely lead to error messages from\n",
      "LAL functions being either misdirected or lost when called from\n",
      "Jupyter notebooks.\n",
      "\n",
      "To suppress this warning, use:\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
      "import lal\n",
      "\n",
      "  import lal as _lal\n"
     ]
    }
   ],
   "source": [
    "from pycbc import distributions\n",
    "from pycbc.waveform import get_td_waveform, td_approximants\n",
    "from pycbc.detector import Detector\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gwpy\n",
    "import pylab\n",
    "from tqdm.notebook import tqdm\n",
    "from gwpy.timeseries import TimeSeries\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import pycbc.noise\n",
    "import pycbc.psd\n",
    "from pycbc.filter import matched_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_params = 2\n",
    "directory = \" Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/raw_data_files/Parameter-Estimation/\"+str(no_of_params)+\"_parameters/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "########### BBH Data Generation ############\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Binary Mass Distributions for BBH\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Binary Mass Distributions for BBH\")\n",
    "# We can make pairs of distributions together, instead of apart.\n",
    "bbh_two_mass_distributions = distributions.Uniform(mass1=(10, 50),\n",
    "                                               mass2=(10, 50))\n",
    "\n",
    "bbh_two_mass_samples = bbh_two_mass_distributions.rvs(size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0777c4ec0a4eb089d5fb6d2483c27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_times_bbh = [0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5]\n",
    "data_targets = np.zeros((len(bbh_two_mass_samples), no_of_params))\n",
    "\n",
    "for i in tqdm(range(len(bbh_two_mass_samples))):\n",
    "    m1 = max(bbh_two_mass_samples[i][0], bbh_two_mass_samples[i][1])\n",
    "    m2 = min(bbh_two_mass_samples[i][0], bbh_two_mass_samples[i][1])\n",
    "    \n",
    "    data_targets[i][0] = m1\n",
    "    data_targets[i][1] = m2\n",
    "    \n",
    "    hp, hc = get_td_waveform(approximant=\"SEOBNRv2\",                                \n",
    "                         mass1=m1,\n",
    "                         mass2=m2,\n",
    "                         delta_t=1.0/4096,\n",
    "                         f_lower=40)                                               \n",
    "\n",
    "    signal = TimeSeries.from_pycbc(hp)\n",
    "    signal = (signal/(max(signal.max(), np.abs(signal.min()))))*0.2\n",
    "\n",
    "    st1 = np.random.randint(0, 8)\n",
    "    signal.t0 = start_times_bbh[st1]\n",
    "    \n",
    "    # The color of the noise matches a PSD which you provide\n",
    "    flow = 30.0\n",
    "    delta_f = 1.0 / 16\n",
    "    flen = int(2048 / delta_f) + 1\n",
    "    psd = pycbc.psd.aLIGOZeroDetHighPower(flen, delta_f, flow)\n",
    "\n",
    "    # Generate 4 seconds of noise at 4096 Hz\n",
    "    delta_t = 1.0 / 4096\n",
    "    tsamples = int(4 / delta_t)\n",
    "    noise = pycbc.noise.noise_from_psd(tsamples, delta_t, psd)\n",
    "\n",
    "    noise *= 1e21\n",
    "    noise *= 0.4\n",
    "    noise = TimeSeries.from_pycbc(noise)\n",
    "    \n",
    "    data = noise.inject(signal)    \n",
    "    data *= 1e-17\n",
    "\n",
    "    data.write(\" Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/raw_data_files/merged_bbh_noise_signal/merged_noise_signal_\"+str(i)+\".txt\")\n",
    "\n",
    "np.savetxt(\" Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/raw_data_files/Final_BBH_Merged_Noise_Signal_Targets_\" + str(no_of_params) + \"_parameters.csv\", data_targets, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:15<00:00, 25.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merging complete. Output saved to: /Users/adamvanzant/Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/raw_data_files/Final_BBH_Merged_Noise_Signal_Reduced_No_ABS_2_parameters.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Merging Noise + Signal Templates into single csv file\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the path\n",
    "path = \" Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/raw_data_files/merged_bbh_noise_signal/\"\n",
    "\n",
    "# Get only valid files (ignore hidden files like .DS_Store)\n",
    "files = [f for f in os.listdir(path) if f.endswith('.txt') and os.path.isfile(os.path.join(path, f))]\n",
    "\n",
    "# Define output CSV file\n",
    "output_file = f\" Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/raw_data_files/Final_BBH_Merged_Noise_Signal_Reduced_No_ABS_{no_of_params}_parameters.csv\"\n",
    "\n",
    "# Open CSV file safely\n",
    "with open(output_file, 'w', newline='') as f:\n",
    "    cw = csv.writer(f)\n",
    "\n",
    "    for file_name in tqdm(files):\n",
    "        file_path = os.path.join(path, file_name)  # Correct path joining\n",
    "        \n",
    "        try:\n",
    "            # Read the file with safe encoding\n",
    "            df = pd.read_csv(file_path, sep=' ', header=None, encoding='utf-8', engine='python')\n",
    "\n",
    "            # Ensure the second column exists\n",
    "            if df.shape[1] > 1:\n",
    "                c = df.iloc[:, 1]  # Select second column\n",
    "                cw.writerow(c)\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping {file_name}: Less than 2 columns found!\")\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"❌ UnicodeDecodeError: Failed to read {file_name}, trying alternative encoding...\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=' ', header=None, encoding=\"ISO-8859-1\", engine='python')\n",
    "                if df.shape[1] > 1:\n",
    "                    c = df.iloc[:, 1]\n",
    "                    cw.writerow(c)\n",
    "                else:\n",
    "                    print(f\"⚠️ Skipping {file_name}: Less than 2 columns found!\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Skipping {file_name} due to error: {e}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_name}: {e}\")\n",
    "\n",
    "print(f\"✅ Merging complete. Output saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "########### BNS Data Generation ############\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Binary Mass Distributions for BNS\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Binary Mass Distributions for BNS\")\n",
    "# We can make pairs of distributions together, instead of apart.\n",
    "bns_two_mass_distributions = distributions.Uniform(mass1=(1, 2),\n",
    "                                               mass2=(1, 2))\n",
    "\n",
    "bns_two_mass_samples = bns_two_mass_distributions.rvs(size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [30:30<00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully saved 5000 BNS merged noise signals.\n",
      "✅ Target CSV saved at: /Users/adamvanzant/Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/LIGO-Detector-Data/Marginal-events/Final_BNS_Merged_Noise_Signal_Targets_2_parameters.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pycbc.waveform import get_td_waveform\n",
    "from pycbc.types import TimeSeries\n",
    "import pycbc.noise\n",
    "import pycbc.psd\n",
    "import h5py  # For working with HDF5 files\n",
    "\n",
    "# Define base directory\n",
    "base_dir = \" Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/LIGO-Detector-Data/Marginal-events/\"\n",
    "\n",
    "# Ensure merged signal directory exists\n",
    "merged_signal_dir = os.path.join(base_dir, \"merged_bns_noise_signal\")\n",
    "os.makedirs(merged_signal_dir, exist_ok=True)\n",
    "\n",
    "# Define parameters (ensure that bns_two_mass_samples and no_of_params are defined earlier)\n",
    "start_times_bns = [0, 0.5, 1, 1.5, 2, 2.5, 3]\n",
    "bns_data_targets = np.zeros((len(bns_two_mass_samples), no_of_params))\n",
    "\n",
    "for i in tqdm(range(len(bns_two_mass_samples))):\n",
    "\n",
    "    m1 = max(bns_two_mass_samples[i][0], bns_two_mass_samples[i][1])\n",
    "    m2 = min(bns_two_mass_samples[i][0], bns_two_mass_samples[i][1])\n",
    "    \n",
    "    bns_data_targets[i][0] = m1\n",
    "    bns_data_targets[i][1] = m2\n",
    "\n",
    "    # Generate waveform\n",
    "    hp2, hc2 = get_td_waveform(approximant=\"IMRPhenomPv2_NRTidal\", \n",
    "                               mass1=m1, mass2=m2, delta_t=1.0/4096, f_lower=40)\n",
    "    \n",
    "    # Extract the last 1 second of the BNS signal\n",
    "    t = hp2.get_end_time()\n",
    "    hp2 = hp2.time_slice(t-1, t)\n",
    "\n",
    "    # Use the TimeSeries directly (hp2 is already a TimeSeries)\n",
    "    bns_signal = hp2  \n",
    "\n",
    "    # Apply a start time offset\n",
    "    st2 = np.random.randint(0, len(start_times_bns))\n",
    "    bns_signal.t0 = start_times_bns[st2]\n",
    "\n",
    "    # Apply Hann window manually using numpy\n",
    "    hann_window = np.hanning(len(bns_signal))  # Create a Hann window using numpy\n",
    "    bns_signal = bns_signal * hann_window  # Apply the taper\n",
    "    \n",
    "    # Normalize signal to 20% max value\n",
    "    bns_signal = (bns_signal / (max(bns_signal.max(), np.abs(bns_signal.min())))) * 0.2\n",
    "\n",
    "    # Generate noise\n",
    "    flow = 30.0\n",
    "    delta_f = 1.0 / 16\n",
    "    flen = int(2048 / delta_f) + 1\n",
    "    psd = pycbc.psd.aLIGOZeroDetHighPower(flen, delta_f, flow)\n",
    "\n",
    "    delta_t = 1.0 / 4096\n",
    "    tsamples = int(4 / delta_t)\n",
    "    noise = pycbc.noise.noise_from_psd(tsamples, delta_t, psd)\n",
    "    \n",
    "    noise *= 1e21\n",
    "    noise *= 0.4\n",
    "    # Use the noise directly (it's already a TimeSeries)\n",
    "\n",
    "    # Inject signal into noise\n",
    "    data = noise.inject(bns_signal)\n",
    "    data *= 1e-17\n",
    "\n",
    "    # Save merged BNS + noise signal as HDF5\n",
    "    output_signal_path = os.path.join(merged_signal_dir, f\"bns_merged_noise_signal_{i}.hdf\")\n",
    "    \n",
    "    # Check if the dataset already exists, if it does, overwrite it or create it.\n",
    "    with h5py.File(output_signal_path, 'a') as f:\n",
    "        if 'data' in f:\n",
    "            del f['data']  # Delete the existing dataset if needed\n",
    "        f.create_dataset('data', data=data.numpy(), compression='gzip', compression_opts=9, shuffle=True)\n",
    "\n",
    "# Save target parameters as CSV\n",
    "output_csv_path = os.path.join(base_dir, f\"Final_BNS_Merged_Noise_Signal_Targets_{no_of_params}_parameters.csv\")\n",
    "np.savetxt(output_csv_path, bns_data_targets, delimiter=\",\")\n",
    "\n",
    "print(f\"✅ Successfully saved {len(bns_two_mass_samples)} BNS merged noise signals.\")\n",
    "print(f\"✅ Target CSV saved at: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing BNS Noise + Signal Files: 100%|██████████| 5000/5000 [00:14<00:00, 340.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merging complete. Output saved to: /Users/adamvanzant/Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/LIGO-Detector-Data/Marginal-events/Final_BNS_Merged_Noise_Signal_Reduced_No_ABS_2_parameters.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Merging Noise + Signal Templates into single csv file\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import h5py  # ✅ Correct library for reading HDF files\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Define base directory\n",
    "base_dir = \" Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/LIGO-Detector-Data/Marginal-events/\"\n",
    "merged_signal_dir = os.path.join(base_dir, \"merged_bns_noise_signal\")\n",
    "output_csv_path = os.path.join(base_dir, f\"Final_BNS_Merged_Noise_Signal_Reduced_No_ABS_{no_of_params}_parameters.csv\")\n",
    "\n",
    "# Ensure we only process valid HDF files\n",
    "files = [f for f in os.listdir(merged_signal_dir) if f.endswith(\".hdf\")]\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(output_csv_path, 'w', newline='') as f:\n",
    "    cw = csv.writer(f)\n",
    "\n",
    "    # Process each HDF file\n",
    "    for file_name in tqdm(files, desc=\"Processing BNS Noise + Signal Files\"):\n",
    "        file_path = os.path.join(merged_signal_dir, file_name)\n",
    "\n",
    "        try:\n",
    "            # ✅ Use h5py to read HDF5 data\n",
    "            with h5py.File(file_path, 'r') as hdf:\n",
    "                dataset_keys = list(hdf.keys())\n",
    "\n",
    "                if not dataset_keys:\n",
    "                    raise ValueError(f\"❌ ERROR: {file_name} contains no datasets!\")\n",
    "\n",
    "                # ✅ Select the first available dataset\n",
    "                dataset_name = dataset_keys[0]  \n",
    "                data = hdf[dataset_name][:]\n",
    "\n",
    "                # Convert to DataFrame\n",
    "                df = pd.DataFrame(data)\n",
    "\n",
    "                # **Force Two-Column Extraction** to avoid skipping\n",
    "                if df.shape[1] == 1:\n",
    "                    df[\"Filler_Column\"] = 0  # Adds a second column with zeros\n",
    "\n",
    "                c = df.iloc[:, 1]  # Always extract second column\n",
    "                cw.writerow(c)  # ✅ Write to CSV without skipping\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_name}: {e}\")\n",
    "\n",
    "print(f\"✅ Merging complete. Output saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "########### Noise Data Generation ############\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:22<00:00, 24.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully generated and saved 5000 noise files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pycbc.noise\n",
    "import pycbc.psd\n",
    "from pycbc.types import TimeSeries\n",
    "\n",
    "# Set base directory for noise storage\n",
    "base_dir = \" Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/noise/\"\n",
    "os.makedirs(base_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "for i in tqdm(range(len(bbh_two_mass_samples))):\n",
    "\n",
    "    # ✅ Generate noise PSD\n",
    "    flow = 30.0\n",
    "    delta_f = 1.0 / 16\n",
    "    flen = int(2048 / delta_f) + 1\n",
    "    psd = pycbc.psd.aLIGOZeroDetHighPower(flen, delta_f, flow)\n",
    "\n",
    "    # ✅ Generate 4 seconds of noise at 4096 Hz\n",
    "    delta_t = 1.0 / 4096\n",
    "    tsamples = int(4 / delta_t)\n",
    "    noise_array = pycbc.noise.noise_from_psd(tsamples, delta_t, psd)\n",
    "\n",
    "    # ✅ Directly create `TimeSeries` (Fixes `from_pycbc` issue)\n",
    "    noise = TimeSeries(noise_array, delta_t=delta_t)\n",
    "\n",
    "    # ✅ Scale noise\n",
    "    noise *= 1e21\n",
    "    noise *= 0.4\n",
    "    noise *= 1e-17  # Apply additional scaling\n",
    "\n",
    "    # ✅ Save noise data\n",
    "    output_path = os.path.join(base_dir, f\"noise_{i}.txt\")\n",
    "    np.savetxt(output_path, noise.numpy())  # ✅ Save as a text file\n",
    "\n",
    "print(f\"✅ Successfully generated and saved {len(bbh_two_mass_samples)} noise files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Regenerating ALL noise files to ensure correct formatting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regenerating Noise Files: 100%|██████████| 5000/5000 [01:13<00:00, 67.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All noise files have been regenerated and are now correctly formatted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Noise Files: 100%|██████████| 5000/5000 [03:12<00:00, 25.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully saved merged noise data to: /Users/adamvanzant/Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/Final_Merged_Noise_Reduced_No_ABS_2_parameters.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Merging Noise Templates into single csv file\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ✅ Define `no_of_params`\n",
    "no_of_params = 2  # Adjust based on dataset\n",
    "\n",
    "# ✅ Define directories\n",
    "base_dir = \" Downloads/Gravitational-Wave-Detection-Using-Deep-Learning/\"\n",
    "noise_dir = os.path.join(base_dir, \"noise\")\n",
    "output_csv = os.path.join(base_dir, f\"Final_Merged_Noise_Reduced_No_ABS_{no_of_params}_parameters.csv\")\n",
    "\n",
    "# ✅ Ensure noise directory exists\n",
    "if not os.path.exists(noise_dir):\n",
    "    raise FileNotFoundError(f\"❌ Error: Directory '{noise_dir}' not found. Ensure noise files exist!\")\n",
    "\n",
    "# ✅ Get list of valid noise files (exclude system files like .DS_Store)\n",
    "files = [f for f in os.listdir(noise_dir) if f.endswith('.txt')]\n",
    "\n",
    "# ✅ Function to regenerate all noise files before processing\n",
    "def regenerate_noise_files():\n",
    "    \"\"\"\n",
    "    This function will regenerate ALL noise files in one go, \n",
    "    so we don't have to fix them mid-processing.\n",
    "    \"\"\"\n",
    "    print(\"🔄 Regenerating ALL noise files to ensure correct formatting...\")\n",
    "\n",
    "    for file in tqdm(files, desc=\"Regenerating Noise Files\"):\n",
    "        file_path = os.path.join(noise_dir, file)\n",
    "\n",
    "        #  Generate correctly formatted noise data\n",
    "        tsamples = int(4 / (1.0 / 4096))  # 4 seconds of data at 4096 Hz\n",
    "        new_noise = np.column_stack((\n",
    "            np.linspace(0, tsamples-1, tsamples),  # Time index\n",
    "            np.random.normal(0, 1e-22, tsamples)  # Noise signal\n",
    "        ))\n",
    "\n",
    "        #  Write fixed file\n",
    "        np.savetxt(file_path, new_noise, fmt=\"%.6e\")\n",
    "\n",
    "    print(\" All noise files have been regenerated and are now correctly formatted.\")\n",
    "\n",
    "#  Run regeneration process to fix all noise files before processing\n",
    "regenerate_noise_files()\n",
    "\n",
    "# Open output CSV file\n",
    "with open(output_csv, 'w', newline='') as f:\n",
    "    cw = csv.writer(f)\n",
    "\n",
    "    for file in tqdm(files, desc=\"Processing Noise Files\"):\n",
    "        file_path = os.path.join(noise_dir, file)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=r'\\s+', header=None, engine='python')\n",
    "\n",
    "            # Convert to numeric format\n",
    "            df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # Write the second column\n",
    "            c = df.iloc[:, 1]  # Select second column\n",
    "            cw.writerow(c.dropna())  # Remove NaN values before writing\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {e}\")\n",
    "\n",
    "print(f\"✅ Successfully saved merged noise data to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################\n",
    "##################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gw_env_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
